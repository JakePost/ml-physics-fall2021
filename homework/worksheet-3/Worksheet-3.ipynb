{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Homework #3\n",
    "### October 27, 2021\n",
    "### Jake Postiglione"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. If you had to eliminate one feature without compromising performance too much, which one would you pick, and why?\n",
    "\n",
    "        If I had to eliminate one feature of the habitability set, it would be the distance from the parent star.\n",
    "        It seems like, with the data we have, just two features of stellar mass and orbital period are enough for the model\n",
    "        to correctly classify new planets.\n",
    "\n",
    "2. In class, we used k = 3 neighbors in the kNN algorithm. Would you recommend increasing the number of neighbors used in the classification for this data set? Why or why not? (Please note that providing the answer without a justification, whether as reasoning or in code, will not get any points!)\n",
    "\n",
    "        For this dataset, I would not recommended a higher value for k. I think, for the kNN algorithm, a careful consideration\n",
    "        needs to be taken into account to make sure your k value is large enough to correctly classify object, but not too large\n",
    "        to the point where unwanted \"noise\" contributes to the classification of an object. I think if the data was extreamly\n",
    "        clustered and clumped together, having a large k value may actually be benificial; however, I think that with this\n",
    "        algorithm in general, the \"boundery lines\" seperating different classifications can contain a number of false positives / negatives.\n",
    "\n",
    "3. Assuming that the maximum Gini impurity is obtained when the objects are uniformly distributed among classes, figure out the maximum Gini impurity in a N-class classification problem.\n",
    "\n",
    "        The maximum gini impurity for any number of features is 0.5, this is becausethe best \"split\" you can make in a data\n",
    "        would only include up to half of the objects being incorrectly classified.\n",
    "\n",
    "4. If you used the two decision trees obtained in the IntroDT notebook from 10/18, using the first 13 and last 13 objects as training set respectively (you can see the notebooks also in the slides from 10/13), to classify the Earth as habitable or not habitable, what verdict would you obtain? (show your thought process!) What can you conclude from this result?\n",
    "\n",
    "        Based on the first decision tree, the Earth would be classified as not habitable since our solar mass is greater than 0.83.\n",
    "\n",
    "        Based on the second decision tree, the Earth would be classified as habitiable. This is because the first check\n",
    "    of stellar mass would return false, the next check of distance would also return false, finally the last check of\n",
    "    solar mass would say that the earth is habitable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extra credit: Write pseudo-code (a sequence of operations, like the one we showed in class for DT) for the kNN algorithm.\n",
    "\n",
    "function knnCLassification(trainingSet, tetData, kValue):\n",
    "    classificationDict\n",
    "\n",
    "    for testObject in testData:\n",
    "        distanceDict\n",
    "\n",
    "        for trainingObject in trainingSet:\n",
    "            distanceDict[trainingObject] = euclideanDist(testObject, trainingObject)\n",
    "\n",
    "        sortAccending(distanceDict)\n",
    "\n",
    "        classOne = 0\n",
    "        classTwo = 0\n",
    "\n",
    "        for i in range(0, kValue):\n",
    "            if distanceDict[i] is classOne:\n",
    "                classOne += 1\n",
    "            else:\n",
    "                classTwo += 1\n",
    "\n",
    "        classificationDict[testObject] = classOne if classOne > classTwo else classTwo\n",
    "\n",
    "    return classificationDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}