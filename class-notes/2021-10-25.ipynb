{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Class Notebook - October 25, 2021\n",
    "### Jake Postiglione\n",
    "\n",
    "## Beyond Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Until now, we have \"judged\" algorithms by the percentage of correct answers.\n",
    "The metric is called __accuracy__ (% of correct answers).\n",
    "\n",
    "Now imagine building an algorithm to look for rare objects (e.g. habitable planets).\n",
    "\n",
    "The data contains 1,000 instances.\n",
    "\n",
    "Of those, 10 belong to the \"interesting\" class.\n",
    "(the data set is imbalanced: the target values are not distributed uniformly).\n",
    "\n",
    "You feel lazy and propose an algorithm that says that there are no habitable planets.\n",
    "\n",
    "What is its accuracy (% of correct predictions)?\n",
    "\n",
    "990/1000\n",
    "99%\n",
    "\n",
    "\"The accuracy paradox\" when you may have a large accuracy score, but the model is useless."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Classifiers Performance: Beyond Accuracy\n",
    "\n",
    "For a binary classifier where the \"true\" or desired class is defined to be positive, every metric is enclosed by four numbers.\n",
    "\n",
    "TP = True Positives\n",
    "\n",
    "TN = True Negatives\n",
    "\n",
    "FP = False Positives\n",
    "\n",
    "FN = False Negatives\n",
    "\n",
    "Accuracy = % of correct predictions\n",
    "\n",
    "TP + TN / (TP + TN + FP + FN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alternative Metrics\n",
    "\n",
    "Precision = percentage of correct positive classifications TP / (TP + FP)\n",
    "\n",
    "Recall = percentage of \"caught\" positive instances = TP / (TP + FN)\n",
    "\n",
    "For our lazy classifier:\n",
    "\n",
    "TP = 0; TN = 990; FP = 0; FN = 10\n",
    "\n",
    "Precision = 0 / (0 + 0) = undefined\n",
    "\n",
    "Recall = 0 / (0 + 10) = undefined\n",
    "\n",
    "In physics (or at least astrophysics), we often talk of precision as purity, or F1 - precision as contamination\n",
    "\n",
    "Recall is best visualized as completeness\n",
    "\n",
    "Another common one is F1-score = weighted avg of precision / recall\n",
    "\n",
    "The best metric can only be decided by you on the basis of the science you want to do."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Precision vs Recall\n",
    "\n",
    "In general, there is a trade-off between precision and recall.\n",
    "\n",
    "It's hard to get both types of errors (FN and FP) down, but you can choose to have fewer errors of one type.\n",
    "\n",
    "High precision: \"Don't waste my time\" (few FP, more FN)\n",
    "\n",
    "High recall: \"Get them all\" (few FN, more FP)\n",
    "\n",
    "    * As told by Cassie Kzyrkov\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrics :Optimizing vs Viewing\n",
    "\n",
    "There is a big difference between choosing the optimal metric\n",
    "(e.g. precision vs accuracy) when comparing models\n",
    "(for example, when we choose between kNN or DT or when we select parameters)\n",
    "and choose the metric that is optimized when building models.\n",
    "\n",
    "For example, in a decision tree, would choosing accuracy or recall chang the optimal splits of a given training set?\n",
    "\n",
    "Why or why not?\n",
    "\n",
    "Many algorithms, especially in sklearn, have a hard coded loss function.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}